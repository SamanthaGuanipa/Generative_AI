{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LtchXS2UJRt0"
      },
      "source": [
        "# Tarea 4: Generative AI"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qe4vfKmNJWrC"
      },
      "source": [
        "1. Analiza un GAN o RNN, repasa el ejemplo, haz tu propia copia del cuaderno y haz anotaciones al respecto.\n",
        "2. Averigua qué hace el código y cómo lo hace (explicando los pasos más importantes).\n",
        "3. Puede añadir \"prints\" adicionales si desea mostrar salidas adicionales.\n",
        "4. Realice uno de los siguientes cambios\n",
        "- learning rate o el tipo de Optimizador (GANs/RNNs)\n",
        "- porcentaje del Dropout (GANs/RNNs; normalmente para RNNs hay 2 valores de dropout)\n",
        "- tamaño de las imágenes (GANs; sólo si el proceso de entrenamiento no dura más de 10 horas)\n",
        "- rediseñar o añadir una capa (GANs; sólo si el proceso de entrenamiento no dura más de\n",
        "10 horas)\n",
        "- tamaño del vocabulario o tamaño de la secuencia utilizada (RNNs; sólo si el proceso de entrenamiento\n",
        "no dura más de 10 minutos)\n",
        "- longitud de incrustación (RNN; sólo si el proceso de entrenamiento no dura más de 10\n",
        "horas)\n",
        "5. Entrenar y evaluar\n",
        "6. Explicar el cambio y los nuevos resultados.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EtwegGJDJ539"
      },
      "source": [
        "# Generación de mi propio texto de Shakespeare\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KkG5hp35Kzm6"
      },
      "source": [
        "Se tiene una frase dada y se deberá predecir la siguiente palabra, la Red Neuronal Recurrente (RNN) tomará la primera palabra de la frase, esta será pasada por una red neuronal y predecirá la siguiente palabra. Sin embargo, para poder predecir la tercera palabra, toma la activación del estado oculto y la segunda palabra como entrada. El mismo proceso continúa y se capta la relación secuencial porque se utiliza el estado oculto de la palabra anterior para predecir la siguiente, lo que significa que de alguna manera se está utilizando una versión codificada de la frase anterior para predecir la palabra siguiente.\n",
        "\n",
        "Existen estructuras más eficaces, como las unidades recurrentes controladas (GRU) y las de memoria a largo plazo (LSTM). El problema práctico de por qué se utilizan las GRU y las LSTM en lugar de las RNN es el siguiente: en las RNN, se utiliza la información de cada palabra anterior para predecir correctamente la palabra siguiente, pero a veces una parte de una frase es suficiente para predecir la palabra siguiente en las LSTM y las GRU.\n",
        "\n",
        "Se utilizará esta idea para diseñar una red de forma que el modelo decida qué palabras seleccionar y escriba como Shakespeare."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CMzaofNVNNVg"
      },
      "source": [
        "# Preprocesamiento"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y_9GkiTZNP-Y"
      },
      "source": [
        "Se creará un dataset que contará con 100 caracteres one-hot encoded como entrada y el output será una versión one-hot encoded del caracter predicho."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Se importan las librerías"
      ],
      "metadata": {
        "id": "V1wfui6G67S5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RNuGCSh3M91h"
      },
      "outputs": [],
      "source": [
        "from __future__ import print_function\n",
        "from keras.callbacks import LambdaCallback\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import LSTM\n",
        "from keras.optimizers import RMSprop\n",
        "from tensorflow.keras.utils import get_file\n",
        "import numpy as np\n",
        "import random\n",
        "import sys\n",
        "import io\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import platform\n",
        "import time\n",
        "import pathlib\n",
        "import os"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Se definen algunas rutas y nombres de archivos relacionados con la descarga y el almacenamiento del conjunto de datos que contiene obras de William Shakespeare."
      ],
      "metadata": {
        "id": "PwTLAChP6_wi"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XT4BwY5jOcl_"
      },
      "outputs": [],
      "source": [
        "# El archivo se almacenará en cache_dir, en la carpeta \"tmp\"\n",
        "cache_dir = './tmp'\n",
        "# Se almacena el nombre del archivo de datos que se descargará, se establece como 'shakespeare.txt'\n",
        "dataset_file_name = 'shakespeare.txt'\n",
        "# URL de origen desde la cual se descargará el archivo de datos\n",
        "dataset_file_origin = 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XYbTef36Odv8",
        "outputId": "ee507e52-9fff-4830-d2c2-446ad1ba6e1e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt\n",
            "1115394/1115394 [==============================] - 0s 0us/step\n"
          ]
        }
      ],
      "source": [
        "'''\n",
        "Se crea una variable llamada dataset_file_path y se inicializa con el valor\n",
        "devuelto por la función tf.keras.utils.get_file(). Esta función se utiliza para\n",
        "descargar un archivo desde una URL y almacenarlo en una ubicación local.\n",
        "Se descarga el archivo desde la URL de origen especificada y lo almacena en el\n",
        "directorio de caché con el nombre especificado, y la variable dataset_file_path\n",
        "contiene la ruta completa del archivo descargado en el sistema de archivos local.\n",
        "'''\n",
        "dataset_file_path = tf.keras.utils.get_file(\n",
        "    fname=dataset_file_name,\n",
        "    origin=dataset_file_origin,\n",
        "    cache_dir=pathlib.Path(cache_dir).absolute()\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UvcP3tP1Ogjv",
        "outputId": "847216c3-e20d-4213-d42e-307e9aa90c99"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/tmp/.keras/datasets/shakespeare.txt\n"
          ]
        }
      ],
      "source": [
        "'''\n",
        "Se abre el archivo de datos descargado, lo lee y lo almacena en una cadena\n",
        "de texto. Luego, crea diccionarios que relacionan caracteres con índices y\n",
        "viceversa, lo que es útil para el procesamiento posterior de los datos.\n",
        "'''\n",
        "\n",
        "print(dataset_file_path)\n",
        "ss = open(dataset_file_path,mode='r')\n",
        "text = ss.read()\n",
        "chars = sorted(list(set(text)))\n",
        "char_indices = dict((c, i) for i, c in enumerate(chars))\n",
        "indices_char = dict((i, c) for i, c in enumerate(chars))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HMWuoZseOkUs",
        "outputId": "a139d60e-828c-448d-9bbd-240178fd9c70"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "number of training examples: 9900\n"
          ]
        }
      ],
      "source": [
        "'''\n",
        "Se crean ejemplos de entrenamiento a partir del texto. Cada ejemplo de\n",
        "entrenamiento consiste en una secuencia de entrada (X) de longitud Tx y\n",
        "la secuencia de salida correspondiente (Y) que es el siguiente carácter en el\n",
        "texto después de la secuencia de entrada.\n",
        "'''\n",
        "def build_data(text, Tx = 100, stride = 1):\n",
        "    X = []\n",
        "    Y = []\n",
        "    for i in range(0, len(text) - Tx, stride):\n",
        "        X.append(text[i: i + Tx])\n",
        "        Y.append(text[i + Tx])\n",
        "    print('number of training examples:', len(X))\n",
        "    return X, Y\n",
        "X,Y = build_data(text[:10000])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_zzFaf6cOqQ0",
        "outputId": "bbc849bf-77e9-4aea-db4b-cbd32f24210e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-6-bbaf2fd114f6>:3: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  x = np.zeros((m, Tx, n_x), dtype=np.bool)\n",
            "<ipython-input-6-bbaf2fd114f6>:4: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  y = np.zeros((m, n_x), dtype=np.bool)\n"
          ]
        }
      ],
      "source": [
        "'''\n",
        "Se transforman las secuencias de texto en matrices numéricas binarias\n",
        "para entrenar un modelo de lenguaje recurrente, donde cada carácter se\n",
        "representa como un vector one-hot en x, y el siguiente carácter se representa\n",
        "como un vector one-hot en y.\n",
        "'''\n",
        "def vectorization(X, Y, n_x, char_indices, Tx = 100):\n",
        "    m = len(X)\n",
        "    x = np.zeros((m, Tx, n_x), dtype=np.bool)\n",
        "    y = np.zeros((m, n_x), dtype=np.bool)\n",
        "    for i, sentence in enumerate(X):\n",
        "        for t, char in enumerate(sentence):\n",
        "            x[i, t, char_indices[char]] = 1\n",
        "        y[i, char_indices[Y[i]]] = 1\n",
        "    return x, y\n",
        "\n",
        "x,y = vectorization(X,Y,len(chars),char_indices,Tx=100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ArAGAOzMOtMY"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "Se introduce aleatoriedad y variabilidad en la generación de texto a partir de\n",
        "un modelo de lenguaje, lo que puede hacer que el texto  sea más interesante y\n",
        "diverso. La temperatura controla la cantidad de aleatoriedad en las predicciones\n",
        "\n",
        "'''\n",
        "def sample(preds, temperature=1.0):\n",
        "    preds = np.asarray(preds).astype('float64')\n",
        "    preds = np.log(preds) / temperature\n",
        "    exp_preds = np.exp(preds)\n",
        "    preds = exp_preds / np.sum(exp_preds)\n",
        "    probas = np.random.multinomial(1, preds, 1)\n",
        "    out = np.random.choice(range(len(chars)), p = probas.ravel())\n",
        "    return out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xw7Lt9dsOxk4"
      },
      "source": [
        "# Desarrollo del modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NJUkxoiDOwf1",
        "outputId": "f70d2dcb-8294-4c36-f64c-2cc0e6cb74b8"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/200\n",
            "20/20 [==============================] - 112s 5s/step - loss: 3.9523\n",
            "Epoch 2/200\n",
            "20/20 [==============================] - 105s 5s/step - loss: 3.2633\n",
            "Epoch 3/200\n",
            "20/20 [==============================] - 105s 5s/step - loss: 3.2584\n",
            "Epoch 4/200\n",
            "20/20 [==============================] - 106s 5s/step - loss: 3.2564\n",
            "Epoch 5/200\n",
            "20/20 [==============================] - 104s 5s/step - loss: 3.2495\n",
            "Epoch 6/200\n",
            "20/20 [==============================] - 104s 5s/step - loss: 3.2464\n",
            "Epoch 7/200\n",
            "20/20 [==============================] - 104s 5s/step - loss: 3.2422\n",
            "Epoch 8/200\n",
            "20/20 [==============================] - 106s 5s/step - loss: 3.2397\n",
            "Epoch 9/200\n",
            "20/20 [==============================] - 107s 5s/step - loss: 3.2353\n",
            "Epoch 10/200\n",
            "20/20 [==============================] - 105s 5s/step - loss: 3.2453\n",
            "Epoch 11/200\n",
            "20/20 [==============================] - 106s 5s/step - loss: 3.2291\n",
            "Epoch 12/200\n",
            "20/20 [==============================] - 105s 5s/step - loss: 3.2350\n",
            "Epoch 13/200\n",
            "20/20 [==============================] - 105s 5s/step - loss: 3.2336\n",
            "Epoch 14/200\n",
            "20/20 [==============================] - 105s 5s/step - loss: 3.2361\n",
            "Epoch 15/200\n",
            "20/20 [==============================] - 105s 5s/step - loss: 3.2304\n",
            "Epoch 16/200\n",
            "20/20 [==============================] - 104s 5s/step - loss: 3.2311\n",
            "Epoch 17/200\n",
            "20/20 [==============================] - 105s 5s/step - loss: 3.2321\n",
            "Epoch 18/200\n",
            "20/20 [==============================] - 106s 5s/step - loss: 3.2286\n",
            "Epoch 19/200\n",
            "20/20 [==============================] - 105s 5s/step - loss: 3.2336\n",
            "Epoch 20/200\n",
            "20/20 [==============================] - 105s 5s/step - loss: 3.2290\n",
            "Epoch 21/200\n",
            "20/20 [==============================] - 105s 5s/step - loss: 3.2280\n",
            "Epoch 22/200\n",
            "20/20 [==============================] - 106s 5s/step - loss: 3.2277\n",
            "Epoch 23/200\n",
            "20/20 [==============================] - 105s 5s/step - loss: 3.2279\n",
            "Epoch 24/200\n",
            "20/20 [==============================] - 104s 5s/step - loss: 3.2268\n",
            "Epoch 25/200\n",
            "20/20 [==============================] - 106s 5s/step - loss: 3.2252\n",
            "Epoch 26/200\n",
            "20/20 [==============================] - 104s 5s/step - loss: 3.2272\n",
            "Epoch 27/200\n",
            "20/20 [==============================] - 104s 5s/step - loss: 3.2278\n",
            "Epoch 28/200\n",
            "20/20 [==============================] - 104s 5s/step - loss: 3.2238\n",
            "Epoch 29/200\n",
            "20/20 [==============================] - 104s 5s/step - loss: 3.2250\n",
            "Epoch 30/200\n",
            "20/20 [==============================] - 105s 5s/step - loss: 3.2393\n",
            "Epoch 31/200\n",
            "20/20 [==============================] - 104s 5s/step - loss: 3.2289\n",
            "Epoch 32/200\n",
            "20/20 [==============================] - 104s 5s/step - loss: 3.2235\n",
            "Epoch 33/200\n",
            "20/20 [==============================] - 105s 5s/step - loss: 3.2241\n",
            "Epoch 34/200\n",
            "20/20 [==============================] - 106s 5s/step - loss: 3.2238\n",
            "Epoch 35/200\n",
            "20/20 [==============================] - 104s 5s/step - loss: 3.2240\n",
            "Epoch 36/200\n",
            "20/20 [==============================] - 104s 5s/step - loss: 3.2238\n",
            "Epoch 37/200\n",
            "20/20 [==============================] - 105s 5s/step - loss: 3.2243\n",
            "Epoch 38/200\n",
            "20/20 [==============================] - 105s 5s/step - loss: 3.2236\n",
            "Epoch 39/200\n",
            "20/20 [==============================] - 104s 5s/step - loss: 3.2236\n",
            "Epoch 40/200\n",
            "20/20 [==============================] - 104s 5s/step - loss: 3.2227\n",
            "Epoch 41/200\n",
            "20/20 [==============================] - 104s 5s/step - loss: 3.2224\n",
            "Epoch 42/200\n",
            "20/20 [==============================] - 105s 5s/step - loss: 3.2226\n",
            "Epoch 43/200\n",
            "20/20 [==============================] - 105s 5s/step - loss: 3.2231\n",
            "Epoch 44/200\n",
            "20/20 [==============================] - 104s 5s/step - loss: 3.2228\n",
            "Epoch 45/200\n",
            "20/20 [==============================] - 104s 5s/step - loss: 3.2233\n",
            "Epoch 46/200\n",
            "20/20 [==============================] - 105s 5s/step - loss: 3.2230\n",
            "Epoch 47/200\n",
            "20/20 [==============================] - 105s 5s/step - loss: 3.2223\n",
            "Epoch 48/200\n",
            "20/20 [==============================] - 105s 5s/step - loss: 3.2231\n",
            "Epoch 49/200\n",
            "20/20 [==============================] - 105s 5s/step - loss: 3.2227\n",
            "Epoch 50/200\n",
            "20/20 [==============================] - 106s 5s/step - loss: 3.2224\n",
            "Epoch 51/200\n",
            "20/20 [==============================] - 105s 5s/step - loss: 3.2222\n",
            "Epoch 52/200\n",
            "20/20 [==============================] - 105s 5s/step - loss: 3.2228\n",
            "Epoch 53/200\n",
            "20/20 [==============================] - 105s 5s/step - loss: 3.2217\n",
            "Epoch 54/200\n",
            "20/20 [==============================] - 106s 5s/step - loss: 3.2223\n",
            "Epoch 55/200\n",
            "20/20 [==============================] - 105s 5s/step - loss: 3.2218\n",
            "Epoch 56/200\n",
            "20/20 [==============================] - 104s 5s/step - loss: 3.2221\n",
            "Epoch 57/200\n",
            "20/20 [==============================] - 104s 5s/step - loss: 3.2215\n",
            "Epoch 58/200\n",
            "20/20 [==============================] - 105s 5s/step - loss: 3.2228\n",
            "Epoch 59/200\n",
            "20/20 [==============================] - 104s 5s/step - loss: 3.2213\n",
            "Epoch 60/200\n",
            "20/20 [==============================] - 104s 5s/step - loss: 3.2224\n",
            "Epoch 61/200\n",
            "20/20 [==============================] - 105s 5s/step - loss: 3.2218\n",
            "Epoch 62/200\n",
            "20/20 [==============================] - 105s 5s/step - loss: 3.2223\n",
            "Epoch 63/200\n",
            "20/20 [==============================] - 105s 5s/step - loss: 3.2217\n",
            "Epoch 64/200\n",
            "20/20 [==============================] - 104s 5s/step - loss: 3.2219\n",
            "Epoch 65/200\n",
            "20/20 [==============================] - 104s 5s/step - loss: 3.2215\n",
            "Epoch 66/200\n",
            "20/20 [==============================] - 105s 5s/step - loss: 3.2212\n",
            "Epoch 67/200\n",
            "20/20 [==============================] - 105s 5s/step - loss: 3.2218\n",
            "Epoch 68/200\n",
            "20/20 [==============================] - 104s 5s/step - loss: 3.2218\n",
            "Epoch 69/200\n",
            "20/20 [==============================] - 104s 5s/step - loss: 3.2222\n",
            "Epoch 70/200\n",
            "20/20 [==============================] - 104s 5s/step - loss: 3.2215\n",
            "Epoch 71/200\n",
            "20/20 [==============================] - 105s 5s/step - loss: 3.2216\n",
            "Epoch 72/200\n",
            "20/20 [==============================] - 104s 5s/step - loss: 3.2218\n",
            "Epoch 73/200\n",
            "20/20 [==============================] - 104s 5s/step - loss: 3.2209\n",
            "Epoch 74/200\n",
            "20/20 [==============================] - 110s 5s/step - loss: 3.2212\n",
            "Epoch 75/200\n",
            "20/20 [==============================] - 104s 5s/step - loss: 3.2213\n",
            "Epoch 76/200\n",
            "20/20 [==============================] - 110s 6s/step - loss: 3.2210\n",
            "Epoch 77/200\n",
            "20/20 [==============================] - 113s 6s/step - loss: 3.2210\n",
            "Epoch 78/200\n",
            "20/20 [==============================] - 107s 5s/step - loss: 3.2209\n",
            "Epoch 79/200\n",
            "20/20 [==============================] - 105s 5s/step - loss: 3.2205\n",
            "Epoch 80/200\n",
            "20/20 [==============================] - 105s 5s/step - loss: 3.2212\n",
            "Epoch 81/200\n",
            "20/20 [==============================] - 113s 6s/step - loss: 3.2206\n",
            "Epoch 82/200\n",
            "20/20 [==============================] - 112s 6s/step - loss: 3.2211\n",
            "Epoch 83/200\n",
            "20/20 [==============================] - 108s 5s/step - loss: 3.2213\n",
            "Epoch 84/200\n",
            "20/20 [==============================] - 106s 5s/step - loss: 3.2207\n",
            "Epoch 85/200\n",
            "20/20 [==============================] - 105s 5s/step - loss: 3.2215\n",
            "Epoch 86/200\n",
            "20/20 [==============================] - 106s 5s/step - loss: 3.2211\n",
            "Epoch 87/200\n",
            "20/20 [==============================] - 105s 5s/step - loss: 3.2212\n",
            "Epoch 88/200\n",
            "20/20 [==============================] - 105s 5s/step - loss: 3.2213\n",
            "Epoch 89/200\n",
            "20/20 [==============================] - 105s 5s/step - loss: 3.2211\n",
            "Epoch 90/200\n",
            "20/20 [==============================] - 106s 5s/step - loss: 3.2207\n",
            "Epoch 91/200\n",
            "20/20 [==============================] - 117s 6s/step - loss: 3.2211\n",
            "Epoch 92/200\n",
            "20/20 [==============================] - 104s 5s/step - loss: 3.2211\n",
            "Epoch 93/200\n",
            "20/20 [==============================] - 111s 6s/step - loss: 3.2211\n",
            "Epoch 94/200\n",
            "20/20 [==============================] - 105s 5s/step - loss: 3.2207\n",
            "Epoch 95/200\n",
            "20/20 [==============================] - 114s 6s/step - loss: 3.2210\n",
            "Epoch 96/200\n",
            "20/20 [==============================] - 107s 5s/step - loss: 3.2201\n",
            "Epoch 97/200\n",
            "20/20 [==============================] - 113s 6s/step - loss: 3.2207\n",
            "Epoch 98/200\n",
            "20/20 [==============================] - 109s 5s/step - loss: 3.2215\n",
            "Epoch 99/200\n",
            "20/20 [==============================] - 105s 5s/step - loss: 3.2208\n",
            "Epoch 100/200\n",
            "20/20 [==============================] - 105s 5s/step - loss: 3.2208\n",
            "Epoch 101/200\n",
            "20/20 [==============================] - 105s 5s/step - loss: 3.2206\n",
            "Epoch 102/200\n",
            "20/20 [==============================] - 104s 5s/step - loss: 3.2209\n",
            "Epoch 103/200\n",
            "20/20 [==============================] - 104s 5s/step - loss: 3.2209\n",
            "Epoch 104/200\n",
            "20/20 [==============================] - 104s 5s/step - loss: 3.2207\n",
            "Epoch 105/200\n",
            "20/20 [==============================] - 105s 5s/step - loss: 3.2203\n",
            "Epoch 106/200\n",
            "20/20 [==============================] - 104s 5s/step - loss: 3.2210\n",
            "Epoch 107/200\n",
            "20/20 [==============================] - 104s 5s/step - loss: 3.2209\n",
            "Epoch 108/200\n",
            "20/20 [==============================] - 115s 6s/step - loss: 3.2204\n",
            "Epoch 109/200\n",
            "20/20 [==============================] - 108s 5s/step - loss: 3.2206\n",
            "Epoch 110/200\n",
            "20/20 [==============================] - 114s 6s/step - loss: 3.2206\n",
            "Epoch 111/200\n",
            "20/20 [==============================] - 114s 6s/step - loss: 3.2205\n",
            "Epoch 112/200\n",
            "20/20 [==============================] - 113s 6s/step - loss: 3.2202\n",
            "Epoch 113/200\n",
            "20/20 [==============================] - 106s 5s/step - loss: 3.2207\n",
            "Epoch 114/200\n",
            "20/20 [==============================] - 104s 5s/step - loss: 3.2204\n",
            "Epoch 115/200\n",
            "20/20 [==============================] - 104s 5s/step - loss: 3.2203\n",
            "Epoch 116/200\n",
            "20/20 [==============================] - 104s 5s/step - loss: 3.2201\n",
            "Epoch 117/200\n",
            "20/20 [==============================] - 105s 5s/step - loss: 3.2203\n",
            "Epoch 118/200\n",
            "20/20 [==============================] - 105s 5s/step - loss: 3.2208\n",
            "Epoch 119/200\n",
            "20/20 [==============================] - 104s 5s/step - loss: 3.2205\n",
            "Epoch 120/200\n",
            "20/20 [==============================] - 104s 5s/step - loss: 3.2205\n",
            "Epoch 121/200\n",
            "20/20 [==============================] - 104s 5s/step - loss: 3.2203\n",
            "Epoch 122/200\n",
            "20/20 [==============================] - 105s 5s/step - loss: 3.2205\n",
            "Epoch 123/200\n",
            "20/20 [==============================] - 104s 5s/step - loss: 3.2203\n",
            "Epoch 124/200\n",
            "20/20 [==============================] - 104s 5s/step - loss: 3.2204\n",
            "Epoch 125/200\n",
            "20/20 [==============================] - 104s 5s/step - loss: 3.2201\n",
            "Epoch 126/200\n",
            "20/20 [==============================] - 104s 5s/step - loss: 3.2200\n",
            "Epoch 127/200\n",
            "20/20 [==============================] - 105s 5s/step - loss: 3.2202\n",
            "Epoch 128/200\n",
            "20/20 [==============================] - 104s 5s/step - loss: 3.2206\n",
            "Epoch 129/200\n",
            "20/20 [==============================] - 104s 5s/step - loss: 3.2202\n",
            "Epoch 130/200\n",
            "20/20 [==============================] - 104s 5s/step - loss: 3.2206\n",
            "Epoch 131/200\n",
            "20/20 [==============================] - 105s 5s/step - loss: 3.2201\n",
            "Epoch 132/200\n",
            "20/20 [==============================] - 104s 5s/step - loss: 3.2205\n",
            "Epoch 133/200\n",
            "20/20 [==============================] - 104s 5s/step - loss: 3.2200\n",
            "Epoch 134/200\n",
            "20/20 [==============================] - 104s 5s/step - loss: 3.2203\n",
            "Epoch 135/200\n",
            "20/20 [==============================] - 104s 5s/step - loss: 3.2197\n",
            "Epoch 136/200\n",
            "20/20 [==============================] - 106s 5s/step - loss: 3.2203\n",
            "Epoch 137/200\n",
            "20/20 [==============================] - 104s 5s/step - loss: 3.2203\n",
            "Epoch 138/200\n",
            "20/20 [==============================] - 104s 5s/step - loss: 3.2200\n",
            "Epoch 139/200\n",
            "20/20 [==============================] - 104s 5s/step - loss: 3.2199\n",
            "Epoch 140/200\n",
            "20/20 [==============================] - 105s 5s/step - loss: 3.2200\n",
            "Epoch 141/200\n",
            "20/20 [==============================] - 104s 5s/step - loss: 3.2202\n",
            "Epoch 142/200\n",
            "20/20 [==============================] - 104s 5s/step - loss: 3.2202\n",
            "Epoch 143/200\n",
            "20/20 [==============================] - 104s 5s/step - loss: 3.2204\n",
            "Epoch 144/200\n",
            "20/20 [==============================] - 104s 5s/step - loss: 3.2201\n",
            "Epoch 145/200\n",
            "20/20 [==============================] - 105s 5s/step - loss: 3.2200\n",
            "Epoch 146/200\n",
            "20/20 [==============================] - 104s 5s/step - loss: 3.2200\n",
            "Epoch 147/200\n",
            "20/20 [==============================] - 104s 5s/step - loss: 3.2199\n",
            "Epoch 148/200\n",
            "20/20 [==============================] - 104s 5s/step - loss: 3.2202\n",
            "Epoch 149/200\n",
            "20/20 [==============================] - 104s 5s/step - loss: 3.2200\n",
            "Epoch 150/200\n",
            "20/20 [==============================] - 100s 5s/step - loss: 3.2202\n",
            "Epoch 151/200\n",
            "20/20 [==============================] - ETA: 0s - loss: 3.2199\n",
            "----- Generating text after Epoch: 150\n",
            "----- diversity: 0.5\n",
            "----- Generating with seed: \" her withal.\n",
            "\n",
            "TRANIO:\n",
            "And here I take the unfeigned oath,\n",
            "Never to marry with her though she would e\"\n",
            " her withal.\n",
            "\n",
            "TRANIO:\n",
            "And here I take the unfeigned oath,\n",
            "Never to marry with her though she would ees t eaa ui ses   or nr  ntn n ao enhstih    ui e uo o  zksoi   t lreeuoneer\n",
            "er hf  ne  dato tte    a eu    t as s wnh ooei  r ah  in ed s   i e o\n",
            "  ee, t elit    ar nac th a i  l   sa n ea     tod a boyh ho t     e ee\n",
            "s  isin  eer e    l a f dener     oo eta l oou e fle ti   h   eW   nseeo  ti e r \n",
            "ite s  ete nhsot e e e ee irt i ess c  reeu t t  o ee hd  l   iw eron e  r esu hetoeht  r rt\n",
            "  oe  ree   rs oe\n",
            "e   s  ttia tle a t iit de  oled i etr    e nu r  ae i ts to r ouoeaosa wn et t t  \n",
            "aye\n",
            "\n",
            "20/20 [==============================] - 153s 8s/step - loss: 3.2199\n",
            "Epoch 152/200\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3079: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "20/20 [==============================] - 101s 5s/step - loss: 3.2199\n",
            "Epoch 153/200\n",
            "20/20 [==============================] - 100s 5s/step - loss: 3.2196\n",
            "Epoch 154/200\n",
            "20/20 [==============================] - 101s 5s/step - loss: 3.2196\n",
            "Epoch 155/200\n",
            "20/20 [==============================] - 102s 5s/step - loss: 3.2205\n",
            "Epoch 156/200\n",
            "20/20 [==============================] - 101s 5s/step - loss: 3.2197\n",
            "Epoch 157/200\n",
            "20/20 [==============================] - 102s 5s/step - loss: 3.2202\n",
            "Epoch 158/200\n",
            "20/20 [==============================] - 101s 5s/step - loss: 3.2197\n",
            "Epoch 159/200\n",
            "20/20 [==============================] - 101s 5s/step - loss: 3.2201\n",
            "Epoch 160/200\n",
            "20/20 [==============================] - 101s 5s/step - loss: 3.2198\n",
            "Epoch 161/200\n",
            "20/20 [==============================] - 100s 5s/step - loss: 3.2204\n",
            "Epoch 162/200\n",
            "20/20 [==============================] - 101s 5s/step - loss: 3.2199\n",
            "Epoch 163/200\n",
            "20/20 [==============================] - 100s 5s/step - loss: 3.2199\n",
            "Epoch 164/200\n",
            "20/20 [==============================] - 102s 5s/step - loss: 3.2198\n",
            "Epoch 165/200\n",
            "20/20 [==============================] - 101s 5s/step - loss: 3.2199\n",
            "Epoch 166/200\n",
            "20/20 [==============================] - 101s 5s/step - loss: 3.2199\n",
            "Epoch 167/200\n",
            "20/20 [==============================] - 101s 5s/step - loss: 3.2200\n",
            "Epoch 168/200\n",
            "20/20 [==============================] - 102s 5s/step - loss: 3.2197\n",
            "Epoch 169/200\n",
            "20/20 [==============================] - 101s 5s/step - loss: 3.2202\n",
            "Epoch 170/200\n",
            "20/20 [==============================] - 102s 5s/step - loss: 3.2198\n",
            "Epoch 171/200\n",
            "20/20 [==============================] - 101s 5s/step - loss: 3.2199\n",
            "Epoch 172/200\n",
            "20/20 [==============================] - 102s 5s/step - loss: 3.2200\n",
            "Epoch 173/200\n",
            "20/20 [==============================] - 102s 5s/step - loss: 3.2198\n",
            "Epoch 174/200\n",
            "20/20 [==============================] - 101s 5s/step - loss: 3.2197\n",
            "Epoch 175/200\n",
            "20/20 [==============================] - 102s 5s/step - loss: 3.2201\n",
            "Epoch 176/200\n",
            "20/20 [==============================] - 101s 5s/step - loss: 3.2198\n",
            "Epoch 177/200\n",
            "20/20 [==============================] - 101s 5s/step - loss: 3.2198\n",
            "Epoch 178/200\n",
            "20/20 [==============================] - 101s 5s/step - loss: 3.2199\n",
            "Epoch 179/200\n",
            "20/20 [==============================] - 102s 5s/step - loss: 3.2198\n",
            "Epoch 180/200\n",
            "20/20 [==============================] - 101s 5s/step - loss: 3.2199\n",
            "Epoch 181/200\n",
            "20/20 [==============================] - 102s 5s/step - loss: 3.2198\n",
            "Epoch 182/200\n",
            "20/20 [==============================] - 101s 5s/step - loss: 3.2198\n",
            "Epoch 183/200\n",
            "20/20 [==============================] - 102s 5s/step - loss: 3.2197\n",
            "Epoch 184/200\n",
            "20/20 [==============================] - 102s 5s/step - loss: 3.2197\n",
            "Epoch 185/200\n",
            "20/20 [==============================] - 101s 5s/step - loss: 3.2198\n",
            "Epoch 186/200\n",
            "20/20 [==============================] - 102s 5s/step - loss: 3.2199\n",
            "Epoch 187/200\n",
            "20/20 [==============================] - 101s 5s/step - loss: 3.2196\n",
            "Epoch 188/200\n",
            "20/20 [==============================] - 102s 5s/step - loss: 3.2198\n",
            "Epoch 189/200\n",
            "20/20 [==============================] - 101s 5s/step - loss: 3.2199\n",
            "Epoch 190/200\n",
            "20/20 [==============================] - 102s 5s/step - loss: 3.2194\n",
            "Epoch 191/200\n",
            "20/20 [==============================] - 102s 5s/step - loss: 3.2197\n",
            "Epoch 192/200\n",
            "20/20 [==============================] - 101s 5s/step - loss: 3.2199\n",
            "Epoch 193/200\n",
            "20/20 [==============================] - 101s 5s/step - loss: 3.2196\n",
            "Epoch 194/200\n",
            "20/20 [==============================] - 101s 5s/step - loss: 3.2197\n",
            "Epoch 195/200\n",
            "20/20 [==============================] - 102s 5s/step - loss: 3.2196\n",
            "Epoch 196/200\n",
            "20/20 [==============================] - 101s 5s/step - loss: 3.2196\n",
            "Epoch 197/200\n",
            "20/20 [==============================] - 101s 5s/step - loss: 3.2195\n",
            "Epoch 198/200\n",
            "20/20 [==============================] - 102s 5s/step - loss: 3.2198\n",
            "Epoch 199/200\n",
            "20/20 [==============================] - 101s 5s/step - loss: 3.2196\n",
            "Epoch 200/200\n",
            "20/20 [==============================] - 103s 5s/step - loss: 3.2196\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7e454c202980>"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "# Se utiliza LSTM\n",
        "model = Sequential()\n",
        "model.add(LSTM(256, input_shape=(100, len(chars)),return_sequences=True))\n",
        "model.add(LSTM(256))\n",
        "model.add(Dense(128,activation='relu'))\n",
        "model.add(Dense(128,activation='relu'))\n",
        "model.add(Dense(len(chars), activation='softmax'))\n",
        "maxlen = 100\n",
        "\n",
        "def on_epoch_end(epoch, _):\n",
        "    if epoch > 0 and epoch % 150 == 0:\n",
        "        print()\n",
        "        print('----- Generating text after Epoch: %d' % epoch)\n",
        "\n",
        "        start_index = random.randint(0, len(text) - maxlen - 1)\n",
        "        for diversity in [0.5]:\n",
        "            print('----- diversity:', diversity)\n",
        "\n",
        "            generated = ''\n",
        "            sentence = text[start_index: start_index + maxlen]\n",
        "            generated += sentence\n",
        "            print('----- Generating with seed: \"' + sentence + '\"')\n",
        "            sys.stdout.write(generated)\n",
        "\n",
        "            for i in range(500):\n",
        "                x_pred = np.zeros((1, maxlen, len(chars)))\n",
        "                for t, char in enumerate(sentence):\n",
        "                    x_pred[0, t, char_indices[char]] = 1.\n",
        "\n",
        "                preds = model.predict(x_pred, verbose=0)[0]\n",
        "                next_index = sample(preds, diversity)\n",
        "                next_char = indices_char[next_index]\n",
        "\n",
        "                sentence = sentence[1:] + next_char\n",
        "\n",
        "                sys.stdout.write(next_char)\n",
        "                sys.stdout.flush()\n",
        "            print()\n",
        "\n",
        "        # Guardar el modelo después de cada generación de texto\n",
        "        model.save('shakespeare_model_after_epoch_{}.h5'.format(epoch))\n",
        "\n",
        "\n",
        "optimizer = RMSprop(learning_rate=0.01)\n",
        "model.compile(loss='categorical_crossentropy', optimizer=optimizer)\n",
        "print_callback = LambdaCallback(on_epoch_end=on_epoch_end)\n",
        "\n",
        "model.fit(x, y,\n",
        "          batch_size=512,\n",
        "          epochs=200,\n",
        "          callbacks=[print_callback])\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Función para generar texto\n",
        "def generate_text(model, seed_text, length, diversity):\n",
        "    generated = ''\n",
        "    sentence = seed_text\n",
        "    generated += sentence\n",
        "\n",
        "    for i in range(length):\n",
        "        x_pred = np.zeros((1, maxlen, len(chars)))\n",
        "        for t, char in enumerate(sentence):\n",
        "            x_pred[0, t, char_indices[char]] = 1.\n",
        "\n",
        "        preds = model.predict(x_pred, verbose=0)[0]\n",
        "        next_index = sample(preds, diversity)\n",
        "        next_char = indices_char[next_index]\n",
        "\n",
        "        sentence = sentence[1:] + next_char\n",
        "        generated += next_char\n",
        "\n",
        "    return generated\n",
        "\n",
        "# Llamar a la función de generación de texto\n",
        "seed_text = \"To be or not to be, that is the question:\"\n",
        "generated_text = generate_text(model, seed_text, length=500, diversity=0.5)\n",
        "print(generated_text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OX4jlXzAwcco",
        "outputId": "68e59501-ae6c-4177-b2a5-ebef1f93f9cf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "To be or not to be, that is the question:i aeih  f ao toie    a eFr  yseoehr liho  h  \n",
            "a t au ottt t eiwl  \n",
            " tetl  a\n",
            "e b   e eit  chsa,t e  e\n",
            "eeo tsi et  il h  ouel ot    edaar e\n",
            " oesn n   ehhiehee\n",
            "eueo  esaee d ire nlre   it d etoh  t timteiu atoeeg msi e   iIta  ioe  t ud te os h  eheo  e  etn tt  sntaio s ca h  r h fhh\n",
            " eehmoie ds etr lyet   coei a t  e se\n",
            "y tnhhl e o t  ah  tee e  s  t  iihe i o  dii fioinru    aa ee   i\n",
            "it epr oim sua  s  n hteen  rnoo o 'mtyoi  n i e e iree o,ee t ntaoea   i,s t   ahl   ketteihtoe \n",
            "oheeatn  t  e \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Se puede observar que el texto generado no tiene ningún sentido o coherencia. Esto puede deberse a el tamaño del modelo, la cantidad de datos de entrenamiento, la diversidad y otros hiperparámetros. En este caso, el texto puede no ser coherente ni representar una continuación lógica de la frase inicial debido a lo difícil que puede ser generar un texto que imite el estilo de Shakespeare."
      ],
      "metadata": {
        "id": "wZp9YnzDxmiK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Se utiliza LSTM para modificar el nuevo modelo y lograr una mejora\n",
        "\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "\n",
        "\n",
        "model2 = Sequential()\n",
        "model2.add(LSTM(256, input_shape=(100, len(chars)),return_sequences=True))\n",
        "model2.add(LSTM(256))\n",
        "model2.add(Dense(128,activation='relu'))\n",
        "model2.add(Dense(128,activation='relu'))\n",
        "model2.add(Dense(len(chars), activation='softmax'))\n",
        "maxlen = 100\n",
        "\n",
        "def on_epoch_end(epoch, _):\n",
        "    if epoch > 0 and epoch % 150 == 0:\n",
        "        print()\n",
        "        print('----- Generating text after Epoch: %d' % epoch)\n",
        "\n",
        "        start_index = random.randint(0, len(text) - maxlen - 1)\n",
        "        for diversity in [0.5]:\n",
        "            print('----- diversity:', diversity)\n",
        "\n",
        "            generated = ''\n",
        "            sentence = text[start_index: start_index + maxlen]\n",
        "            generated += sentence\n",
        "            print('----- Generating with seed: \"' + sentence + '\"')\n",
        "            sys.stdout.write(generated)\n",
        "\n",
        "            for i in range(500):\n",
        "                x_pred = np.zeros((1, maxlen, len(chars)))\n",
        "                for t, char in enumerate(sentence):\n",
        "                    x_pred[0, t, char_indices[char]] = 1.\n",
        "\n",
        "                preds = model.predict(x_pred, verbose=0)[0]\n",
        "                next_index = sample(preds, diversity)\n",
        "                next_char = indices_char[next_index]\n",
        "\n",
        "                sentence = sentence[1:] + next_char\n",
        "\n",
        "                sys.stdout.write(next_char)\n",
        "                sys.stdout.flush()\n",
        "            print()\n",
        "\n",
        "        # Guardar el modelo después de cada generación de texto\n",
        "        model.save('shakespeare_model_after_epoch_{}.h5'.format(epoch))\n",
        "\n",
        "# Cambiar el optimizador a Adam\n",
        "optimizer = Adam(learning_rate=0.01)\n",
        "\n",
        "model2.compile(loss='categorical_crossentropy', optimizer=optimizer)\n",
        "\n",
        "\n",
        "print_callback = LambdaCallback(on_epoch_end=on_epoch_end)\n",
        "\n",
        "model.fit(x, y, batch_size=512, epochs=200, callbacks=[print_callback])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 250
        },
        "id": "XyeNP-ekx6Qf",
        "outputId": "57f224d4-4b52-4cb7-9714-62ab10c81792"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-f79f292bc422>\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mmodel2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSequential\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mmodel2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLSTM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchars\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mreturn_sequences\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mmodel2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLSTM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'Sequential' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "seed_text = \"To be or not to be, that is the question:\"\n",
        "generated_text = generate_text(model2, seed_text, length=500, diversity=0.5)\n",
        "print(generated_text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 214
        },
        "id": "5IRlQh23CQ_8",
        "outputId": "6e1fc78b-4200-420b-e8cf-825d20cced2c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-40f4bc49b918>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mseed_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"To be or not to be, that is the question:\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mgenerated_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed_text\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlength\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdiversity\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerated_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'generate_text' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Aunque se cambió el optimizador no se vieron resultados que parezcan mejores, puede ser que mejore cambiando el learning rate aparte del optimizador. Debo seguir haciendo cambios para ver qué puede hacer que mejore el modelo."
      ],
      "metadata": {
        "id": "SolbGgz25dJy"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}